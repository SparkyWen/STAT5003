# w9

| Aspect                          | Gradient Boosting (GBM)                                      | XGBoost (Extreme Gradient Boosting)                          | AdaBoost (Adaptive Boosting)                                 |
| ------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Core idea                       | View boosting as **gradient descent in function space**: each new tree fits the **negative gradient** of a loss function | A **highly optimized implementation of gradient boosting trees**, with extra regularization and system-level tricks | Reweight training examples: each new learner focuses more on **previously misclassified samples** |
| Loss function                   | **General differentiable loss**: e.g. squared error, logistic loss, etc. | Same gradient boosting idea, but uses **second-order Taylor expansion (first + second derivatives)** of loss for splits | Traditionally minimizes **exponential loss**, implicitly via sample reweighting |
| How it updates                  | At iteration (m), fit a new tree to the **residuals / negative gradient** of the loss w.r.t. current predictions | Same as GBM conceptually, but uses **first & second-order gradients**, plus **regularization** in the tree objective | After each weak learner, **increase weights of misclassified points**, decrease weights of correctly classified ones |
| Base learner                    | Usually **small decision trees** (e.g. depth 3–5), but can be any weak learner | **Decision trees** with many engineering tricks: approximate splits, column subsampling, etc. | Often **decision stumps** (depth-1 trees), or shallow trees  |
| Regularization                  | Basic GBM has **shrinkage (learning rate)**, **tree depth control**, **subsampling**(stochastic GBM) | Strong built-in regularization: **L1/L2 penalties on leaf weights**, **tree complexity penalty**, **column & row subsampling** | Little explicit regularization; can be quite **sensitive to noise/outliers** because exponential loss heavily penalizes them |
| Handling missing values / speed | Depends on implementation; standard GBM can be slower and less optimized | Designed for **speed and scale**: parallelization, cache-aware, efficient handling of missing values and sparse features | Simple algorithm; fast on small/medium data, but not as optimized or scalable as XGBoost |
| Typical usage                   | Generic gradient boosting in libraries (e.g. `gbm` in R, `GradientBoosting*` in scikit-learn) | Real-world competitions, large tabular data, Kaggle etc.; often top choice for **tabular predictive tasks** | Earlier classic method for binary classification; still used for teaching and simple problems |

Gradient Boosting 是“用梯度一步步减损失”，XGBoost 是“把 Gradient Boosting 工程化+正则化到极致”，AdaBoost 是“每轮调样本权重，专盯上次分错的样本”。

**GBM**：

- 有学习率、子采样、树深度等基本控制手段，但实现层面一般比较“朴素”。

**XGBoost**：

- 专门为“**大规模表格数据 + 高性能**”设计：
  - 列采样、行采样、稀疏优化、并行、缓存友好
  - 内置 **L1/L2 正则、树结构惩罚**
- 你可以简单记：**“工程版加强型 Gradient Boosting”**。

**AdaBoost**：

- 算法本身很简洁，但缺乏复杂的正则设计；
- 对噪声和异常点比较敏感（错得越惨权重越高，容易被“噪声牵着走”）。



## 1. 加载包 & 数据准备（Hitters 数据集）

```
##-----------------------------
## 1. 加载需要的 R 包
##-----------------------------
library(caret)      # 数据划分、建模工具箱
library(ISLR2)      # 提供 Hitters 数据集（ISLR 第2版）
library(leaps)      # regsubsets()：逐步变量选择
library(glmnet)     # Lasso / Ridge 回归
library(kableExtra) # 漂亮的表格（可选）

##-----------------------------
## 2. 读取并清洗 Hitters 数据
##-----------------------------
data(Hitters, package = "ISLR2")  # 载入数据
Hitters <- na.omit(Hitters)       # 删除含 NA 的样本行
```

> 记住：Hitters 是一个棒球工资数据集，响应变量是 `Salary`。

------

## 2. Forward Selection（前向逐步回归）

### 2.1 使用 `regsubsets()` 做前向选择

```
##-----------------------------
## 3. 前向逐步选择（Forward selection）
##   - nvmax = 21：最多允许 21 个自变量
##   - method = "forward"：前向选择
##-----------------------------
step_fwd <- regsubsets(
  Salary ~ .,
  data   = Hitters,
  nvmax  = 21,                # 最大模型大小（最多几个预测变量）
  method = "forward"          # 前向逐步选择
)

## 查看 regsubsets 的结果摘要（包含 Cp、BIC、调整R²等）
step_summary <- summary(step_fwd)
```

`summary(step_fwd)` 里常用的几个向量：

- `step_summary$cp`：每个模型大小对应的 **Cp**
- `step_summary$bic`：对应的 **BIC**
- `step_summary$adjr2`：对应的 **调整 R²**

### 2.2 选出最佳模型大小（按 Cp / BIC / 调整 R²）

```
##-----------------------------
## 4. 把各个指标整理成表格
##-----------------------------
model_summary <- data.frame(
  num_vars = 1:21,                # 模型包含的变量个数
  Cp      = step_summary$cp,
  BIC     = step_summary$bic,
  adjR2   = step_summary$adjr2
)

## 找到：
## - 使 Cp 最小的模型大小
## - 使 BIC 最小的模型大小
## - 使调整 R² 最大的模型大小
best_cp    <- model_summary[which.min(model_summary$Cp), ]
best_bic   <- model_summary[which.min(model_summary$BIC), ]
best_adjr2 <- model_summary[which.max(model_summary$adjR2), ]

##（可选）用 kable 输出为表格
# kable(best_cp)
# kable(best_bic)
# kable(best_adjr2)
```

> 考点：
>
> - **Forward selection** 是从空模型开始，每次加1个变量，直到达到 nvmax 或不再改善指标。
> - 你需要能解释：**“选 Cp 最小 / BIC 最小 / 调整 R² 最大的模型”**的含义。

------

## 3. Lasso 回归（α = 1）

### 3.1 构造 X 矩阵 & y 向量 + 训练/测试划分

```
##-----------------------------
## 5. 构造模型矩阵（去掉截距列）
##-----------------------------
y <- Hitters$Salary
X <- model.matrix(Salary ~ ., Hitters)[, -1]  # 去掉第一列截距

set.seed(5003)
inTrain <- createDataPartition(y, p = 0.7, list = FALSE)
X_train <- X[inTrain, ]
y_train <- y[inTrain]
X_test  <- X[-inTrain, ]
y_test  <- y[-inTrain]
```

### 3.2 设定 λ 搜索网格，并拟合 Lasso 路径

```
##-----------------------------
## 6. 设定 lambda 搜索网格
##   - 10^4 到 10^-2 之间取 100 个值
##-----------------------------
grid <- 10 ^ seq(4, -2, length = 100)

##-----------------------------
## 7. 拟合 Lasso 回归（alpha = 1）
##-----------------------------
lasso.mod <- glmnet(
  X_train, y_train,
  alpha       = 1,       # Lasso
  lambda      = grid,    # 一系列候选 lambda
  standardize = TRUE     # 标准化各个自变量
)

## 可视化：系数随 log(lambda) 变化的路径图
plot(lasso.mod, xvar = "lambda", label = TRUE)
```

> 图中常见考点：
>
> - λ 越大 → 惩罚越强 → **更多系数被压到 0**。
> - Lasso 可以做变量选择（部分系数=0）。

### 3.3 用交叉验证选最佳 λ（`cv.glmnet`）

```
##-----------------------------
## 8. 交叉验证选择最佳 lambda
##-----------------------------
set.seed(5003)
cv.out <- cv.glmnet(
  X_train, y_train,
  alpha  = 1,          # Lasso
  lambda = grid,       # 同一个 lambda 网格
  nfolds = 10          # 10 折交叉验证
)

plot(cv.out)           # 纵轴是 CV MSE，横轴是 log(lambda)

## 提取 CV MSE 最小的 lambda
bestlam_lasso <- cv.out$lambda.min
bestlam_lasso
```

### 3.4 提取最佳模型的系数 & 测试集 MSE

```
##-----------------------------
## 9. 查看最佳 lambda 下的系数
##-----------------------------
best.betas_lasso <- coef(cv.out, s = "lambda.min")
best.betas_lasso  # 哪些变量系数被压成 0？

##-----------------------------
## 10. 在测试集上计算 Lasso 的 MSE
##-----------------------------
lasso.pred <- predict(lasso.mod,
                      s    = bestlam_lasso,
                      newx = X_test)

lasso_test_MSE <- mean((lasso.pred - y_test)^2)
lasso_test_MSE
```

> 需要会解释：
>
> - `coef(..., s="lambda.min")`：在选出的最优 λ 下的系数向量。
> - 哪些系数为 0 → 那些变量被 Lasso “删掉”。
> - **Test MSE** 是衡量泛化性能的指标。

------

## 4. Ridge 回归（α = 0）

### 4.1 拟合 Ridge 路径

```
##-----------------------------
## 11. Ridge 回归（alpha = 0）
##-----------------------------
ridge.mod <- glmnet(
  X_train, y_train,
  alpha       = 0,     # Ridge
  lambda      = grid,
  standardize = TRUE
)

plot(ridge.mod, xvar = "lambda", label = TRUE)
```

> Ridge 和 Lasso 的区别：
>
> - Ridge：**不会把系数压到正好为 0**，只是变小。
> - 更适合“很多变量都多少有用”的情况。

### 4.2 CV 选最佳 λ，并在测试集上评估

```
##-----------------------------
## 12. 交叉验证选择 Ridge 的最佳 lambda
##-----------------------------
set.seed(5003)
cv.out.ridge <- cv.glmnet(
  X_train, y_train,
  alpha  = 0,     # Ridge
  lambda = grid,
  nfolds = 10
)

plot(cv.out.ridge)

bestlam_ridge <- cv.out.ridge$lambda.min
bestlam_ridge

##-----------------------------
## 13. 在测试集上计算 Ridge 的 MSE
##-----------------------------
ridge.pred <- predict(ridge.mod,
                      s    = bestlam_ridge,
                      newx = X_test)

ridge_test_MSE <- mean((ridge.pred - y_test)^2)
ridge_test_MSE
```