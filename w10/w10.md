# w10

## 1. 加载包（Loading packages）

```
# ---------------------------
# 1. 加载本周会用到的所有包
# ---------------------------
library(tidyverse)   # 数据清洗和管道操作（%>% 等）
library(ggplot2)     # 画图
library(here)        # 构造相对路径，保证在不同电脑路径仍然正确
library(caret)       # 数据划分、建模辅助（createDataPartition 等）

library(tree)        # 决策树模型
library(ranger)      # 随机森林（更快、现代化的实现）
library(randomForest) # 经典随机森林实现（本周主要看 ranger）
library(gbm)         # Gradient Boosting Machine
library(xgboost)     # XGBoost，常用高性能 Boosting 库
```

------

## 2. 读取数据 + 划分训练 / 测试集

```
# ---------------------------
# 2. 读取红酒质量数据集
# ---------------------------
wine <- read.csv(
  here("datasets", "winequality-data.csv"),  # 使用 here() 保证路径正确
  header = TRUE
)

# 一般最后一列是 quality（目标变量），前面是特征
# 如果数据集中最后一列是别的 ID / 不需要的列，这里会去掉
wine <- wine[, -ncol(wine)]   # 删除最后一列（根据文件结构而定）

# ---------------------------
# 3. 划分训练集 / 测试集（50% / 50%）
# ---------------------------
set.seed(5003)  # 设随机种子，保证每次划分一致（可复现）

# 按照 quality 的分布分层抽样，p = 0.5 表示抽 50% 做训练
inTrain <- createDataPartition(wine[["quality"]], p = 0.5)[[1]]

wine.train <- wine[inTrain, ]      # 训练集
wine.test  <- wine[-inTrain, ]     # 测试集
```

**你要记住的点：**

- `createDataPartition(y, p)`：**按 y 的分布分层抽样**，适合分类/回归任务。
- 训练 / 测试集划分写法在后续所有模型中是通用模板。

------

## 3. 决策树（Decision Tree，`tree()`）

### 3.1 拟合决策树 & 可视化

```
# ---------------------------
# 4. 拟合一个回归树：quality ~ 所有其他变量
# ---------------------------
tree.model <- tree(
  quality ~ .,         # 目标是 quality，"." 表示用其余所有变量作为自变量
  data = wine.train
)

# 查看树模型的结构和摘要
summary(tree.model)

# 画出决策树结构
plot(tree.model)
text(tree.model, pretty = 0)  # 在树节点上标出分裂变量和阈值
```

### 3.2 计算训练 / 测试 RSS（误差）

```
# ---------------------------
# 5. 决策树模型在训练集上的预测与 RSS
# ---------------------------
tree.train <- predict(tree.model, newdata = wine.train)     # 训练集预测
RSS.train.tree <- sum((tree.train - wine.train$quality)^2)  # 训练集 RSS

# ---------------------------
# 6. 决策树模型在测试集上的预测与 RSS
# ---------------------------
tree.test <- predict(tree.model, newdata = wine.test)       # 测试集预测
RSS.test.tree <- sum((tree.test - wine.test$quality)^2)     # 测试集 RSS
```

> 考试时要会：**用 sum((y_hat - y_true)^2) 计算 RSS**，并比较不同模型/超参数下的 RSS 大小。

------

## 4. 随机森林（Random Forest，`ranger()`）

### 4.1 固定 mtry，调节 num.trees（树的数量）

```
# ---------------------------
# 7. 用 ranger 拟合随机森林并考察树数目对 RSS 的影响
# ---------------------------

RSS.train.rf <- c()   # 保存每个 num.trees 下训练 RSS
RSS.test.rf  <- c()   # 保存每个 num.trees 下测试 RSS

ntree.seq <- seq(from = 50, to = 1000, by = 50)  # 树数量的取值序列

for (i in ntree.seq) {
  rf.model <- ranger(
    quality ~ ., 
    data      = wine.train,
    num.trees = i,          # 当前循环的树的数量
    mtry      = 6,          # 每次分裂考虑的变量个数（这里先固定为 6）
    importance = "impurity" # 记录变量重要性（基于不纯度降低）
  )
  
  # --- 训练集 RSS ---
  rf.train.pred <- predict(rf.model, data = wine.train)$predictions
  RSS.train.rf  <- c(RSS.train.rf,
                     sum((rf.train.pred - wine.train$quality)^2))
  
  # --- 测试集 RSS ---
  rf.test.pred <- predict(rf.model, data = wine.test)$predictions
  RSS.test.rf  <- c(RSS.test.rf,
                    sum((rf.test.pred - wine.test$quality)^2))
}

# ---------------------------
# 8. 画出 num.trees vs Test RSS
# ---------------------------
plot(
  ntree.seq, RSS.test.rf,
  type = "b",                 # 连线+点
  xlab = "Number of Trees",   # 横轴：树的数量
  ylab = "Test RSS"           # 纵轴：测试集 RSS
)
```

### 4.2 查看变量重要性（可选）

```
# ---------------------------
# 9. 查看变量重要性（impurity-based）
# ---------------------------
rf.model.final <- ranger(
  quality ~ ., data = wine.train,
  num.trees = 500, mtry = 6,
  importance = "impurity"
)

# 按重要性排序
sort(rf.model.final$variable.importance, decreasing = TRUE)
```

> 关键概念：
>
> - `num.trees`：树越多，**方差一般下降**，但计算更慢。
> - `mtry`：每次分裂考虑的特征数。较小的 mtry ⇒ 更随机，通常更去相关、方差更低。
> - `importance`：用于**解释模型、找重要变量**。

------

## 5. Boosting – GBM（`gbm()`）

### 5.1 调节 n.trees 看 Test RSS

```
# ---------------------------
# 10. 用 gbm 做 Boosting 并调节 n.trees
# ---------------------------
RSS.test.gbm <- c()    # 保存不同 n.trees 下的 Test RSS

ntree.seq <- seq(from = 50, to = 2000, by = 50)  # Boosting 迭代次数

for (i in ntree.seq) {
  gbm.model <- gbm(
    quality ~ ., 
    data         = wine.train,
    distribution = "gaussian", # 回归问题 ⇒ 高斯分布
    n.trees      = i           # Boosting 迭代次数
    # 讲义里可能还会设置 shrinkage, interaction.depth 等，这里先用默认
  )
  
  gbm.test.pred <- predict(
    gbm.model,
    newdata = wine.test,
    n.trees = i
  )
  
  RSS.gbm <- sum((gbm.test.pred - wine.test$quality)^2)
  RSS.test.gbm <- c(RSS.test.gbm, RSS.gbm)
}

# ---------------------------
# 11. 画出 n.trees vs Test RSS
# ---------------------------
plot(
  ntree.seq, RSS.test.gbm,
  type = "b",
  xlab = "Number of Trees (ntrees)",
  ylab = "Test RSS"
)
```

> 核心点：
>
> - GBM 是 **前向阶段式加法模型**，一棵棵小树叠加。
> - `n.trees` 增大 ⇒ 训练拟合能力增强，但测试误差可能先降后升（过拟合）。

------

## 6. Boosting – XGBoost（`xgboost()`）

### 6.1 网格搜索：eta（学习率） × nrounds（迭代次数）

```
# ---------------------------
# 12. 使用 xgboost 对 eta 和 nrounds 做简单网格搜索
# ---------------------------

# 初始化矩阵：4 个 nrounds × 3 个 eta
RSS.test.xgb <- matrix(0, nrow = 4, ncol = 3)  

j <- 1   # 列索引，对应不同 eta

for (eta in c(0.01, 0.02, 0.03)) {    # 学习率
  i <- 1                               # 行索引，对应不同 nrounds
  
  for (nrounds in c(300, 500, 1000, 1500)) {   # 迭代次数
    xgb.model <- xgboost(
      data  = as.matrix(wine.train[, -12]),   # 自变量矩阵（去掉第 12 列 quality）
      label = wine.train[, 12],               # 响应变量 quality
      nrounds   = nrounds,                    # 迭代轮数
      max_depth = 2,                          # 每棵树的最大深度（较小以防过拟合）
      eta       = eta,                        # 学习率
      verbose   = FALSE
    )
    
    xgb.test <- predict(
      xgb.model,
      newdata = as.matrix(wine.test[, -12])
    )
    
    RSS.xgb <- sum((xgb.test - wine.test$quality)^2)
    RSS.test.xgb[i, j] <- RSS.xgb
    
    i <- i + 1   # 换下一个 nrounds
  }
  
  j <- j + 1     # 换下一个 eta
}

# ---------------------------
# 13. 画出不同 eta 下 Test RSS vs nrounds
# ---------------------------
nrounds.grid <- c(300, 500, 1000, 1500)

plot(
  x = nrounds.grid,
  y = RSS.test.xgb[, 1],  # 第一列，对应 eta = 0.01
  col = "red",
  type = "b",
  ylim = c(950, 1230),
  xlab = "Number of Boosting Iterations",
  ylab = "Test RSS"
)

lines(nrounds.grid, RSS.test.xgb[, 2], col = "blue",  type = "b")  # eta = 0.02
lines(nrounds.grid, RSS.test.xgb[, 3], col = "green", type = "b")  # eta = 0.03

legend(
  "topright",
  legend = c("eta = 0.01", "eta = 0.02", "eta = 0.03"),
  col = c("red", "blue", "green"),
  lty = 1, pch = 1
)
```

> 核心理解：

- **eta（学习率）** 越小，每一步更新更“谨慎”，一般需要更大的 `nrounds` 才能达到类似拟合程度，过拟合风险相对小。
- **nrounds** 越大，模型复杂度越高，可能先降误差后过拟合。
- 用双层 `for` 循环做简单 **网格搜索 (grid search)**，并记录每组超参数的 Test RSS。